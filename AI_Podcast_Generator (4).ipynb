{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# AI Podcast Generator - Two Hosts Conversation\n",
        "\n",
        "This notebook generates podcasts with two AI hosts having a natural conversation about any topic you provide.\n",
        "\n",
        "**Features:**\n",
        "- Uses Mistral 7B for intelligent conversation generation\n",
        "- VibeVoice for real-time text-to-speech\n",
        "- Two distinct voices for each host\n",
        "- Cloudflared for public access\n",
        "- Beautiful Gradio UI\n",
        "\n",
        "**Requirements:** T4 GPU or better"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step1",
      "metadata": {},
      "source": [
        "## Step 1: Setup Environment & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"\\u2705 GPU detected: {gpu_name}\")\n",
        "    if \"T4\" not in gpu_name and \"A100\" not in gpu_name and \"V100\" not in gpu_name:\n",
        "        print(\"\\u26a0\\ufe0f Warning: For best performance, use T4 GPU or better\")\n",
        "else:\n",
        "    print(\"\"\"\n",
        "    \\u26a0\\ufe0f WARNING: No GPU detected!\n",
        "    \n",
        "    To enable GPU:\n",
        "    1. Click 'Runtime' > 'Change runtime type'\n",
        "    2. Select 'T4 GPU'\n",
        "    3. Click 'Save'\n",
        "    \"\"\")\n",
        "\n",
        "# Clone VibeVoice repository\n",
        "![ -d /content/VibeVoice ] || git clone --quiet --branch main --depth 1 https://github.com/microsoft/VibeVoice.git /content/VibeVoice\n",
        "print(\"\\u2705 Cloned VibeVoice repository\")\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers accelerate bitsandbytes gradio scipy\n",
        "!pip install -q uv 2>/dev/null || true\n",
        "!uv pip install --system -e /content/VibeVoice 2>/dev/null || pip install -q -e /content/VibeVoice\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared && chmod +x cloudflared\n",
        "print(\"\\u2705 Installed dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step2",
      "metadata": {},
      "source": [
        "## Step 2: Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download_models",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# Download VibeVoice TTS model\n",
        "print(\"Downloading VibeVoice TTS model...\")\n",
        "snapshot_download(\"microsoft/VibeVoice-Realtime-0.5B\", local_dir=\"/content/models/VibeVoice-Realtime-0.5B\")\n",
        "print(\"\\u2705 Downloaded VibeVoice TTS model\")\n",
        "\n",
        "# Load Mistral 7B for conversation\n",
        "print(\"\\nLoading Mistral 7B for conversation generation...\")\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Use 4-bit quantization to fit in T4 GPU memory\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"\\u2705 Loaded Mistral 7B model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step3",
      "metadata": {},
      "source": [
        "## Step 3: Setup VibeVoice TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_tts",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Add VibeVoice to path\n",
        "sys.path.insert(0, '/content/VibeVoice')\n",
        "\n",
        "# Import the correct model and processor classes\n",
        "from vibevoice.modular.modeling_vibevoice_streaming_inference import VibeVoiceStreamingForConditionalGenerationInference\n",
        "from vibevoice.processor.vibevoice_streaming_processor import VibeVoiceStreamingProcessor\n",
        "\n",
        "print(\"Loading VibeVoice TTS model...\")\n",
        "tts_model_path = \"/content/models/VibeVoice-Realtime-0.5B\"\n",
        "\n",
        "# Load the processor\n",
        "tts_processor = VibeVoiceStreamingProcessor.from_pretrained(tts_model_path)\n",
        "\n",
        "# Load the model\n",
        "try:\n",
        "    tts_model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
        "        tts_model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"cuda\",\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Flash attention not available, falling back to SDPA: {e}\")\n",
        "    tts_model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
        "        tts_model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"cuda\",\n",
        "        attn_implementation=\"sdpa\",\n",
        "    )\n",
        "\n",
        "tts_model.eval()\n",
        "\n",
        "# Configure the noise scheduler\n",
        "tts_model.model.noise_scheduler = tts_model.model.noise_scheduler.from_config(\n",
        "    tts_model.model.noise_scheduler.config,\n",
        "    algorithm_type=\"sde-dpmsolver++\",\n",
        "    beta_schedule=\"squaredcos_cap_v2\",\n",
        ")\n",
        "tts_model.set_ddpm_inference_steps(num_steps=5)\n",
        "\n",
        "print(\"\\u2705 Loaded VibeVoice TTS model\")\n",
        "\n",
        "# Find available voices\n",
        "voice_dir = Path(\"/content/VibeVoice/demo/voices/streaming_model\")\n",
        "voice_presets = {}\n",
        "if voice_dir.exists():\n",
        "    for pt_path in voice_dir.glob(\"*.pt\"):\n",
        "        voice_presets[pt_path.stem] = pt_path\n",
        "    print(f\"\\u2705 Found {len(voice_presets)} voices: {list(voice_presets.keys())}\")\n",
        "else:\n",
        "    print(\"\\u26a0\\ufe0f Voice directory not found\")\n",
        "\n",
        "# Select two different voices for hosts\n",
        "voice_list = list(voice_presets.keys())\n",
        "HOST1_VOICE = voice_list[0] if len(voice_list) > 0 else \"en-Carter_man\"\n",
        "HOST2_VOICE = voice_list[1] if len(voice_list) > 1 else HOST1_VOICE\n",
        "\n",
        "# Try to pick a female voice for variety if available\n",
        "for v in voice_list:\n",
        "    if \"woman\" in v.lower() or \"emma\" in v.lower() or \"grace\" in v.lower():\n",
        "        HOST2_VOICE = v\n",
        "        break\n",
        "\n",
        "print(f\"\\nHost 1 (Alex) voice: {HOST1_VOICE}\")\n",
        "print(f\"Host 2 (Sam) voice: {HOST2_VOICE}\")\n",
        "\n",
        "# Cache loaded voice presets\n",
        "voice_cache = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step4",
      "metadata": {},
      "source": [
        "## Step 4: Define Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "core_functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\nimport threading\nfrom vibevoice.modular.streamer import AudioStreamer\n\ndef load_voice_preset(voice_name):\n    \"\"\"Load and cache a voice preset.\"\"\"\n    if voice_name in voice_cache:\n        return voice_cache[voice_name]\n    \n    if voice_name not in voice_presets:\n        print(f\"Voice {voice_name} not found, using {HOST1_VOICE}\")\n        voice_name = HOST1_VOICE\n    \n    voice_path = voice_presets[voice_name]\n    prefilled_outputs = torch.load(voice_path, map_location=\"cuda\", weights_only=False)\n    voice_cache[voice_name] = prefilled_outputs\n    return prefilled_outputs\n\n\ndef generate_conversation(topic, num_exchanges=5):\n    \"\"\"Generate a podcast conversation between two hosts about a topic.\"\"\"\n    \n    system_prompt = f\"\"\"Write a natural podcast conversation between two hosts about: {topic}\n\nHost 1 is named Alex - enthusiastic, curious, asks great questions.\nHost 2 is named Sam - knowledgeable, provides insights, shares anecdotes.\n\nWrite exactly {num_exchanges} back-and-forth exchanges.\nEach line must start with either ALEX: or SAM: followed by their actual dialogue.\nDo NOT include any placeholders, brackets, or stage directions.\nJust write what they actually say out loud.\nKeep each response to 2-3 sentences maximum.\nMake it engaging and conversational.\n\"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": system_prompt}]\n    inputs = llm_tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n    \n    with torch.no_grad():\n        outputs = llm_model.generate(\n            inputs,\n            max_new_tokens=1024,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=llm_tokenizer.eos_token_id\n        )\n    \n    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Parse the conversation\n    lines = response.split('\\n')\n    conversation = []\n    \n    for line in lines:\n        line = line.strip()\n        if line.startswith('ALEX:'):\n            text = line.replace('ALEX:', '').strip()\n            # Skip placeholder lines or empty lines\n            if text and '[' not in text and text.lower() != 'dialogue':\n                conversation.append(('Alex', text, HOST1_VOICE))\n        elif line.startswith('SAM:'):\n            text = line.replace('SAM:', '').strip()\n            # Skip placeholder lines or empty lines\n            if text and '[' not in text and text.lower() != 'dialogue':\n                conversation.append(('Sam', text, HOST2_VOICE))\n    \n    return conversation\n\n\ndef clean_text_for_tts(text):\n    \"\"\"Clean text before sending to TTS - remove parenthetical stage directions.\"\"\"\n    import re\n    # Remove stage directions like (laughs), (sighs), etc.\n    text = re.sub(r'\\([^)]*\\)', '', text)\n    # Clean up extra spaces\n    text = ' '.join(text.split())\n    return text.strip()\n\n\ndef text_to_speech(text, voice_name):\n    \"\"\"Convert text to speech using VibeVoice.\"\"\"\n    # Clean the text first\n    text = clean_text_for_tts(text)\n    \n    if not text or not text.strip():\n        return None, None\n    \n    try:\n        # Load voice preset\n        prefilled_outputs = load_voice_preset(voice_name)\n        \n        # Clean text\n        text = text.strip().replace(\"'\", \"'\")\n        \n        # Prepare inputs using the processor\n        processed = tts_processor.process_input_with_cached_prompt(\n            text=text,\n            cached_prompt=prefilled_outputs,\n            padding=True,\n            return_tensors=\"pt\",\n            return_attention_mask=True,\n        )\n        \n        # Move to device\n        inputs = {k: v.to(\"cuda\") if hasattr(v, 'to') else v for k, v in processed.items()}\n        \n        # Setup streaming\n        audio_streamer = AudioStreamer(batch_size=1, stop_signal=None, timeout=None)\n        stop_event = threading.Event()\n        errors = []\n        \n        def run_generation():\n            try:\n                tts_model.generate(\n                    **inputs,\n                    max_new_tokens=None,\n                    cfg_scale=1.5,\n                    tokenizer=tts_processor.tokenizer,\n                    generation_config={\"do_sample\": False, \"temperature\": 1.0, \"top_p\": 1.0},\n                    audio_streamer=audio_streamer,\n                    stop_check_fn=stop_event.is_set,\n                    verbose=False,\n                    refresh_negative=True,\n                    all_prefilled_outputs=copy.deepcopy(prefilled_outputs),\n                )\n            except Exception as e:\n                errors.append(e)\n                import traceback\n                traceback.print_exc()\n            finally:\n                audio_streamer.end()\n        \n        # Start generation in thread\n        gen_thread = threading.Thread(target=run_generation, daemon=True)\n        gen_thread.start()\n        \n        # Collect audio chunks\n        all_audio = []\n        try:\n            stream = audio_streamer.get_stream(0)\n            for audio_chunk in stream:\n                if torch.is_tensor(audio_chunk):\n                    audio_chunk = audio_chunk.detach().cpu().to(torch.float32).numpy()\n                else:\n                    audio_chunk = np.asarray(audio_chunk, dtype=np.float32)\n                \n                if audio_chunk.ndim > 1:\n                    audio_chunk = audio_chunk.reshape(-1)\n                \n                # Normalize chunk\n                peak = np.max(np.abs(audio_chunk)) if audio_chunk.size else 0.0\n                if peak > 1.0:\n                    audio_chunk = audio_chunk / peak\n                \n                all_audio.append(audio_chunk.astype(np.float32))\n        finally:\n            stop_event.set()\n            audio_streamer.end()\n            gen_thread.join(timeout=30)\n        \n        if errors:\n            print(f\"Generation error: {errors[0]}\")\n            return None, None\n        \n        if all_audio:\n            audio = np.concatenate(all_audio)\n            return audio, 24000\n        else:\n            print(\"No audio chunks collected\")\n            return None, None\n            \n    except Exception as e:\n        print(f\"TTS Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\n\nprint(\"\\u2705 Core functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step5",
      "metadata": {},
      "source": [
        "## Step 5: Test TTS (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_tts",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test TTS with a simple phrase\n",
        "print(\"Testing TTS with Host 1 voice...\")\n",
        "test_audio, sr = text_to_speech(\"Hello, welcome to our podcast!\", HOST1_VOICE)\n",
        "\n",
        "if test_audio is not None:\n",
        "    print(f\"\\u2705 TTS working! Audio length: {len(test_audio)/sr:.2f} seconds\")\n",
        "    # Play audio in Colab\n",
        "    from IPython.display import Audio, display\n",
        "    display(Audio(test_audio, rate=sr))\n",
        "else:\n",
        "    print(\"\\u274c TTS test failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step6",
      "metadata": {},
      "source": [
        "## Step 6: Launch Podcast Generator UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gradio_ui",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\nimport subprocess\nimport threading\nimport re\nimport time\n\ndef generate_podcast(topic, num_exchanges, host1_voice, host2_voice, progress=gr.Progress()):\n    \"\"\"Main function to generate the podcast.\"\"\"\n    \n    progress(0, desc=\"Generating conversation script...\")\n    \n    # Generate the conversation with selected voices\n    conversation = generate_conversation_with_voices(topic, int(num_exchanges), host1_voice, host2_voice)\n    \n    if not conversation:\n        return \"Failed to generate conversation. Please try again.\", None\n    \n    # Display the script\n    script = \"\\n\\n\".join([f\"**{speaker}:** {text}\" for speaker, text, _ in conversation])\n    \n    progress(0.2, desc=\"Generating audio...\")\n    \n    # Generate audio for each line\n    all_audio = []\n    sample_rate = 24000\n    \n    for i, (speaker, text, voice) in enumerate(conversation):\n        progress((0.2 + 0.7 * (i / len(conversation))), desc=f\"Generating audio for {speaker} ({i+1}/{len(conversation)})...\")\n        \n        audio, sr = text_to_speech(text, voice)\n        if audio is not None:\n            all_audio.append(audio)\n            sample_rate = sr\n            # Add pause between speakers (0.5 seconds)\n            pause = np.zeros(int(sample_rate * 0.5))\n            all_audio.append(pause)\n    \n    if all_audio:\n        progress(0.95, desc=\"Finalizing...\")\n        \n        # Combine all audio\n        combined_audio = np.concatenate(all_audio)\n        \n        # Normalize\n        if np.abs(combined_audio).max() > 0:\n            combined_audio = combined_audio / np.abs(combined_audio).max() * 0.9\n        \n        # Return as tuple for Gradio Audio (sample_rate, audio_array)\n        progress(1.0, desc=\"Done!\")\n        return f\"**Podcast Generated!**\\n\\n---\\n\\n{script}\", (sample_rate, combined_audio)\n    else:\n        return f\"Audio generation failed.\\n\\n---\\n\\n{script}\", None\n\n\ndef generate_conversation_with_voices(topic, num_exchanges, host1_voice, host2_voice):\n    \"\"\"Generate conversation using the selected voices.\"\"\"\n    system_prompt = f\"\"\"You are a podcast script writer. Write a complete podcast episode script about: {topic}\n\nThe podcast has two hosts:\n- ALEX: The main host who introduces the show, welcomes listeners, and asks questions\n- SAM: The co-host who provides expert insights and explanations\n\nIMPORTANT STRUCTURE:\n1. ALEX must START by welcoming listeners to the podcast and introducing today's topic\n2. They discuss the topic naturally for {num_exchanges} exchanges total\n3. ALEX must END by thanking Sam and the listeners, and signing off\n\nFORMAT RULES:\n- Each line starts with ALEX: or SAM: followed by what they say\n- Write ONLY their spoken words, no stage directions or brackets\n- Keep each turn to 1-3 sentences\n- Make it sound natural and conversational\n\nExample opening:\nALEX: Hey everyone, welcome back to the show! Today we're diving into something really exciting.\nSAM: That's right, and I can't wait to break this down for our listeners.\n\nNow write the complete podcast script about: {topic}\n\"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": system_prompt}]\n    inputs = llm_tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n    \n    with torch.no_grad():\n        outputs = llm_model.generate(\n            inputs,\n            max_new_tokens=1500,\n            do_sample=True,\n            temperature=0.8,\n            top_p=0.9,\n            pad_token_id=llm_tokenizer.eos_token_id\n        )\n    \n    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    lines = response.split('\\n')\n    conversation = []\n    \n    for line in lines:\n        line = line.strip()\n        if line.startswith('ALEX:'):\n            text = line.replace('ALEX:', '').strip()\n            if text and '[' not in text and text.lower() != 'dialogue':\n                conversation.append(('Alex', text, host1_voice))\n        elif line.startswith('SAM:'):\n            text = line.replace('SAM:', '').strip()\n            if text and '[' not in text and text.lower() != 'dialogue':\n                conversation.append(('Sam', text, host2_voice))\n    \n    return conversation\n\n\n# Get voice options\nvoice_options = list(voice_presets.keys()) if voice_presets else [\"en-Carter_man\"]\n\n# Create Gradio Interface\nwith gr.Blocks(title=\"AI Podcast Generator\", theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"\"\"\n    # AI Podcast Generator\n    \n    Generate engaging podcast conversations between two AI hosts!\n    Enter a topic, pick voices for each host, and create your podcast.\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            topic_input = gr.Textbox(\n                label=\"Podcast Topic\",\n                placeholder=\"e.g., 'The future of AI', 'Why coffee is amazing', 'Space exploration'\",\n                lines=3\n            )\n            \n            num_exchanges = gr.Slider(\n                minimum=2, maximum=10, value=4, step=1,\n                label=\"Number of Exchanges\",\n                info=\"How many back-and-forth exchanges between hosts\"\n            )\n            \n            gr.Markdown(\"### Voice Selection\")\n            \n            with gr.Row():\n                host1_voice = gr.Dropdown(\n                    choices=voice_options,\n                    value=HOST1_VOICE,\n                    label=\"Alex's Voice (Host 1)\",\n                    info=\"Curious, enthusiastic host\"\n                )\n                host2_voice = gr.Dropdown(\n                    choices=voice_options,\n                    value=HOST2_VOICE,\n                    label=\"Sam's Voice (Host 2)\", \n                    info=\"Knowledgeable, insightful host\"\n                )\n            \n            generate_btn = gr.Button(\"Generate Podcast\", variant=\"primary\", size=\"lg\")\n        \n        with gr.Column(scale=1):\n            audio_output = gr.Audio(\n                label=\"Generated Podcast\",\n                type=\"numpy\",\n                interactive=False,\n                autoplay=False\n            )\n            script_output = gr.Markdown(label=\"Conversation Script\")\n    \n    gr.Markdown(f\"*Powered by Mistral 7B + VibeVoice TTS*\")\n    \n    generate_btn.click(\n        fn=generate_podcast, \n        inputs=[topic_input, num_exchanges, host1_voice, host2_voice], \n        outputs=[script_output, audio_output]\n    )\n\nprint(\"Starting server...\")\n\n# Launch Gradio\nthreading.Thread(target=lambda: demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False, quiet=True), daemon=True).start()\ntime.sleep(3)\n\n# Start cloudflared tunnel\ncf = subprocess.Popen(\n    \"./cloudflared tunnel --url http://localhost:7860 --no-autoupdate\",\n    shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1\n)\n\nurl_pattern = re.compile(r\"(https://[a-z0-9-]+\\.trycloudflare\\.com)\")\nprint(\"Looking for public URL...\")\n\nfor line in cf.stdout:\n    m = url_pattern.search(line)\n    if m:\n        print(f\"\\nYour Podcast Generator is live at:\\n\\n{m.group(1)}\\n\")\n        break\n\nprint(\"--- Press Stop to end ---\")\nwhile True:\n    time.sleep(1)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "AI_Podcast_Generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}